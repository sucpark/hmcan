{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longformer Document Classification (VS Code + Colab Extension)\n",
    "\n",
    "## Phase 3: Long Document Classification\n",
    "\n",
    "Handle documents up to 4096 tokens with efficient attention:\n",
    "- Longformer (Sliding Window + Global Attention)\n",
    "- BigBird (Block Sparse Attention)\n",
    "\n",
    "**Compatible with VS Code Colab Extension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU (need at least 15GB for Longformer)\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training Settings ===\n",
    "MAX_SAMPLES = 5000\n",
    "MAX_LENGTH = 2048  # Reduce if OOM (out of memory)\n",
    "BATCH_SIZE = 2     # Small batch due to memory\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# === Logging Settings ===\n",
    "USE_WANDB = False  # Set True to enable WandB logging\n",
    "WANDB_API_KEY = \"\"  # Paste your API key here if using WandB\n",
    "WANDB_PROJECT = \"hmcan\"\n",
    "\n",
    "print(f\"Max samples: {MAX_SAMPLES}\")\n",
    "print(f\"Max length: {MAX_LENGTH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers>=4.30.0 -q\n",
    "!pip install datasets>=2.14.0 -q\n",
    "!pip install wandb -q\n",
    "!pip install accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    LongformerModel,\n",
    "    LongformerTokenizer,\n",
    "    BigBirdModel,\n",
    "    BigBirdTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Weights & Biases Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB setup (programmatic login - no interactive prompt)\n",
    "if USE_WANDB and WANDB_API_KEY:\n",
    "    import wandb\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "    print(\"WandB logged in successfully!\")\n",
    "elif USE_WANDB:\n",
    "    print(\"Warning: USE_WANDB=True but no API key provided. WandB will be disabled.\")\n",
    "    USE_WANDB = False\n",
    "else:\n",
    "    print(\"WandB disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Dataset\n",
    "\n",
    "For long document experiments, we use IMDB dataset (longer reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset (longer reviews than Yelp)\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "train_data = dataset['train'].shuffle(seed=42).select(range(min(MAX_SAMPLES, len(dataset['train']))))\n",
    "test_data = dataset['test'].shuffle(seed=42).select(range(min(MAX_SAMPLES // 5, len(dataset['test']))))\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "# Check text lengths\n",
    "lengths = [len(x['text'].split()) for x in train_data]\n",
    "print(f\"\\nText lengths (words):\")\n",
    "print(f\"  Mean: {sum(lengths)/len(lengths):.0f}\")\n",
    "print(f\"  Max: {max(lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Longformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Longformer tokenizer\n",
    "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "class LongformerDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=4096):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            item['text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Global attention mask: 1 for [CLS] token, 0 for others\n",
    "        global_attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n",
    "        global_attention_mask[0] = 1  # [CLS] token gets global attention\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'global_attention_mask': global_attention_mask,\n",
    "            'label': torch.tensor(item['label'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongformerClassifier(nn.Module):\n",
    "    \"\"\"Longformer for long document classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, global_attention_mask):\n",
    "        outputs = self.longformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask,\n",
    "        )\n",
    "        # Use [CLS] token representation\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. BigBird Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BigBird tokenizer\n",
    "bigbird_tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
    "\n",
    "class BigBirdDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=4096):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            item['text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(item['label'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigBirdClassifier(nn.Module):\n",
    "    \"\"\"BigBird for long document classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bigbird = BigBirdModel.from_pretrained(\n",
    "            'google/bigbird-roberta-base',\n",
    "            attention_type='block_sparse',  # or 'original_full'\n",
    "            block_size=64,\n",
    "            num_random_blocks=3,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bigbird(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_longformer(model, loader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        global_attention_mask = batch['global_attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask, global_attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{correct/total:.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_longformer(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Evaluating'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        global_attention_mask = batch['global_attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask, global_attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def train_epoch_bigbird(model, loader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{correct/total:.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_bigbird(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Evaluating'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "train_ds = LongformerDataset(train_data, longformer_tokenizer, max_length=MAX_LENGTH)\n",
    "test_ds = LongformerDataset(test_data, longformer_tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb if enabled\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        name='longformer-classifier',\n",
    "        config={\n",
    "            'model': 'longformer-base-4096',\n",
    "            'max_length': MAX_LENGTH,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'learning_rate': 2e-5,\n",
    "            'epochs': NUM_EPOCHS,\n",
    "            'dataset': 'imdb',\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Initialize model\n",
    "model = LongformerClassifier(num_classes=2).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "best_acc = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch_longformer(\n",
    "        model, train_loader, optimizer, scheduler, criterion, device\n",
    "    )\n",
    "    test_loss, test_acc = evaluate_longformer(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train/loss': train_loss,\n",
    "            'train/accuracy': train_acc,\n",
    "            'val/loss': test_loss,\n",
    "            'val/accuracy': test_acc,\n",
    "        })\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), 'longformer_classifier_best.pt')\n",
    "        print(f\"Saved best model with accuracy: {best_acc:.4f}\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n",
    "\n",
    "print(f\"\\nBest Test Accuracy (Longformer): {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train BigBird (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Prepare BigBird data\n",
    "train_ds_bb = BigBirdDataset(train_data, bigbird_tokenizer, max_length=MAX_LENGTH)\n",
    "test_ds_bb = BigBirdDataset(test_data, bigbird_tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "train_loader_bb = DataLoader(train_ds_bb, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_bb = DataLoader(test_ds_bb, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb if enabled\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        name='bigbird-classifier',\n",
    "        config={\n",
    "            'model': 'bigbird-roberta-base',\n",
    "            'max_length': MAX_LENGTH,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'learning_rate': 2e-5,\n",
    "            'epochs': NUM_EPOCHS,\n",
    "            'dataset': 'imdb',\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Initialize BigBird model\n",
    "model_bb = BigBirdClassifier(num_classes=2).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_bb = AdamW(model_bb.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler_bb = get_linear_schedule_with_warmup(\n",
    "    optimizer_bb,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "best_acc_bb = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch_bigbird(\n",
    "        model_bb, train_loader_bb, optimizer_bb, scheduler_bb, criterion, device\n",
    "    )\n",
    "    test_loss, test_acc = evaluate_bigbird(\n",
    "        model_bb, test_loader_bb, criterion, device\n",
    "    )\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train/loss': train_loss,\n",
    "            'train/accuracy': train_acc,\n",
    "            'val/loss': test_loss,\n",
    "            'val/accuracy': test_acc,\n",
    "        })\n",
    "    \n",
    "    if test_acc > best_acc_bb:\n",
    "        best_acc_bb = test_acc\n",
    "        torch.save(model_bb.state_dict(), 'bigbird_classifier_best.pt')\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n",
    "\n",
    "print(f\"\\nBest Test Accuracy (BigBird): {best_acc_bb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Phase 3 Results: Long Document Classification\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<25} {'Max Length':>12} {'Test Accuracy':>15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'BERT (Phase 2)':<25} {'512':>12} {'~88%':>15}\")\n",
    "try:\n",
    "    print(f\"{'Longformer':<25} {f'{MAX_LENGTH}':>12} {f'{best_acc*100:.2f}%':>15}\")\n",
    "except:\n",
    "    print(f\"{'Longformer':<25} {f'{MAX_LENGTH}':>12} {'Not trained':>15}\")\n",
    "try:\n",
    "    print(f\"{'BigBird':<25} {f'{MAX_LENGTH}':>12} {f'{best_acc_bb*100:.2f}%':>15}\")\n",
    "except:\n",
    "    print(f\"{'BigBird':<25} {f'{MAX_LENGTH}':>12} {'Not trained':>15}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results (Git Push)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved models\n",
    "!ls -la *.pt 2>/dev/null || echo \"No model files found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Clone repo and save models there for git push\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/sucpark/hmcan.git\"\n",
    "PROJECT_DIR = \"/content/hmcan\"\n",
    "\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    !git clone {REPO_URL} {PROJECT_DIR}\n",
    "\n",
    "# Copy models to repo\n",
    "!mkdir -p {PROJECT_DIR}/outputs/longformer_phase3\n",
    "!cp longformer_classifier_best.pt {PROJECT_DIR}/outputs/longformer_phase3/ 2>/dev/null || echo \"No Longformer model\"\n",
    "!cp bigbird_classifier_best.pt {PROJECT_DIR}/outputs/longformer_phase3/ 2>/dev/null || echo \"No BigBird model\"\n",
    "\n",
    "print(f\"Models copied to {PROJECT_DIR}/outputs/longformer_phase3/\")\n",
    "print(\"To push: cd to repo, git add, commit, push\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Memory & Speed Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_inference_time(model, loader, device, num_batches=10, model_type='longformer'):\n",
    "    model.eval()\n",
    "    times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            start = time.time()\n",
    "            if model_type == 'longformer' and 'global_attention_mask' in batch:\n",
    "                global_attention_mask = batch['global_attention_mask'].to(device)\n",
    "                _ = model(input_ids, attention_mask, global_attention_mask)\n",
    "            else:\n",
    "                _ = model(input_ids, attention_mask)\n",
    "            torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    return sum(times) / len(times) * 1000  # ms per batch\n",
    "\n",
    "# Measure (if models are loaded)\n",
    "try:\n",
    "    longformer_time = measure_inference_time(model, test_loader, device, model_type='longformer')\n",
    "    print(f\"Longformer inference: {longformer_time:.2f} ms/batch\")\n",
    "except:\n",
    "    print(\"Longformer model not available for timing\")\n",
    "\n",
    "try:\n",
    "    bigbird_time = measure_inference_time(model_bb, test_loader_bb, device, model_type='bigbird')\n",
    "    print(f\"BigBird inference: {bigbird_time:.2f} ms/batch\")\n",
    "except:\n",
    "    print(\"BigBird model not available for timing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
