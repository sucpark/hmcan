{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Longformer Document Classification on Google Colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sucpark/hmcan/blob/main/notebooks/train_longformer_colab.ipynb)\n",
        "\n",
        "## Phase 3: Long Document Classification\n",
        "\n",
        "Handle documents up to 4096 tokens with efficient attention:\n",
        "- Longformer (Sliding Window + Global Attention)\n",
        "- BigBird (Block Sparse Attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU (need at least 15GB for Longformer)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install transformers>=4.30.0 -q\n",
        "!pip install datasets>=2.14.0 -q\n",
        "!pip install wandb -q\n",
        "!pip install accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    LongformerModel,\n",
        "    LongformerTokenizer,\n",
        "    BigBirdModel,\n",
        "    BigBirdTokenizer,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import wandb\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Weights & Biases Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Dataset\n",
        "\n",
        "For long document experiments, we'll use datasets with longer texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load IMDB dataset (longer reviews than Yelp)\n",
        "dataset = load_dataset('imdb')\n",
        "\n",
        "# Sample for memory constraints\n",
        "MAX_SAMPLES = 5000\n",
        "\n",
        "train_data = dataset['train'].shuffle(seed=42).select(range(min(MAX_SAMPLES, len(dataset['train']))))\n",
        "test_data = dataset['test'].shuffle(seed=42).select(range(min(MAX_SAMPLES // 5, len(dataset['test']))))\n",
        "\n",
        "print(f\"Train samples: {len(train_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")\n",
        "\n",
        "# Check text lengths\n",
        "lengths = [len(x['text'].split()) for x in train_data]\n",
        "print(f\"\\nText lengths (words):\")\n",
        "print(f\"  Mean: {sum(lengths)/len(lengths):.0f}\")\n",
        "print(f\"  Max: {max(lengths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Longformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Longformer tokenizer\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "\n",
        "class LongformerDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=4096):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            item['text'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        # Global attention mask: 1 for [CLS] token, 0 for others\n",
        "        global_attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n",
        "        global_attention_mask[0] = 1  # [CLS] token gets global attention\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'global_attention_mask': global_attention_mask,\n",
        "            'label': torch.tensor(item['label'])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LongformerClassifier(nn.Module):\n",
        "    \"\"\"Longformer for long document classification.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(768, num_classes)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, global_attention_mask):\n",
        "        outputs = self.longformer(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            global_attention_mask=global_attention_mask,\n",
        "        )\n",
        "        # Use [CLS] token representation\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. BigBird Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load BigBird tokenizer\n",
        "bigbird_tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
        "\n",
        "class BigBirdDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=4096):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            item['text'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(item['label'])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BigBirdClassifier(nn.Module):\n",
        "    \"\"\"BigBird for long document classification.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.bigbird = BigBirdModel.from_pretrained(\n",
        "            'google/bigbird-roberta-base',\n",
        "            attention_type='block_sparse',  # or 'original_full'\n",
        "            block_size=64,\n",
        "            num_random_blocks=3,\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(768, num_classes)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bigbird(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch_longformer(model, loader, optimizer, scheduler, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(loader, desc='Training')\n",
        "    for batch in pbar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        global_attention_mask = batch['global_attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask, global_attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        \n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{correct/total:.4f}'})\n",
        "    \n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_longformer(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch in tqdm(loader, desc='Evaluating'):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        global_attention_mask = batch['global_attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        logits = model(input_ids, attention_mask, global_attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    \n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "\n",
        "def train_epoch_bigbird(model, loader, optimizer, scheduler, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(loader, desc='Training')\n",
        "    for batch in pbar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        \n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{correct/total:.4f}'})\n",
        "    \n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_bigbird(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch in tqdm(loader, desc='Evaluating'):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    \n",
        "    return total_loss / len(loader), correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train Longformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data (use smaller max_length for memory)\n",
        "MAX_LENGTH = 2048  # Reduce if OOM\n",
        "BATCH_SIZE = 2     # Small batch due to memory\n",
        "\n",
        "train_ds = LongformerDataset(train_data, longformer_tokenizer, max_length=MAX_LENGTH)\n",
        "test_ds = LongformerDataset(test_data, longformer_tokenizer, max_length=MAX_LENGTH)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project='hmcan',\n",
        "    name='longformer-classifier',\n",
        "    config={\n",
        "        'model': 'longformer-base-4096',\n",
        "        'max_length': MAX_LENGTH,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': 2e-5,\n",
        "        'epochs': 3,\n",
        "        'dataset': 'imdb',\n",
        "    }\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "model = LongformerClassifier(num_classes=2).to(device)\n",
        "\n",
        "# Freeze most of Longformer to save memory (optional)\n",
        "# for param in model.longformer.embeddings.parameters():\n",
        "#     param.requires_grad = False\n",
        "# for i, layer in enumerate(model.longformer.encoder.layer):\n",
        "#     if i < 10:  # Freeze first 10 layers\n",
        "#         for param in layer.parameters():\n",
        "#             param.requires_grad = False\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "total_steps = len(train_loader) * 3\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "best_acc = 0\n",
        "for epoch in range(3):\n",
        "    print(f\"\\nEpoch {epoch + 1}/3\")\n",
        "    \n",
        "    train_loss, train_acc = train_epoch_longformer(\n",
        "        model, train_loader, optimizer, scheduler, criterion, device\n",
        "    )\n",
        "    test_loss, test_acc = evaluate_longformer(\n",
        "        model, test_loader, criterion, device\n",
        "    )\n",
        "    \n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "    \n",
        "    wandb.log({\n",
        "        'epoch': epoch + 1,\n",
        "        'train/loss': train_loss,\n",
        "        'train/accuracy': train_acc,\n",
        "        'val/loss': test_loss,\n",
        "        'val/accuracy': test_acc,\n",
        "    })\n",
        "    \n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save(model.state_dict(), 'longformer_classifier_best.pt')\n",
        "        print(f\"Saved best model with accuracy: {best_acc:.4f}\")\n",
        "\n",
        "wandb.finish()\n",
        "print(f\"\\nBest Test Accuracy (Longformer): {best_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train BigBird (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear GPU memory\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Prepare BigBird data\n",
        "train_ds_bb = BigBirdDataset(train_data, bigbird_tokenizer, max_length=MAX_LENGTH)\n",
        "test_ds_bb = BigBirdDataset(test_data, bigbird_tokenizer, max_length=MAX_LENGTH)\n",
        "\n",
        "train_loader_bb = DataLoader(train_ds_bb, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader_bb = DataLoader(test_ds_bb, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project='hmcan',\n",
        "    name='bigbird-classifier',\n",
        "    config={\n",
        "        'model': 'bigbird-roberta-base',\n",
        "        'max_length': MAX_LENGTH,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': 2e-5,\n",
        "        'epochs': 3,\n",
        "        'dataset': 'imdb',\n",
        "    }\n",
        ")\n",
        "\n",
        "# Initialize BigBird model\n",
        "model_bb = BigBirdClassifier(num_classes=2).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer_bb = AdamW(model_bb.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "scheduler_bb = get_linear_schedule_with_warmup(\n",
        "    optimizer_bb,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "best_acc_bb = 0\n",
        "for epoch in range(3):\n",
        "    print(f\"\\nEpoch {epoch + 1}/3\")\n",
        "    \n",
        "    train_loss, train_acc = train_epoch_bigbird(\n",
        "        model_bb, train_loader_bb, optimizer_bb, scheduler_bb, criterion, device\n",
        "    )\n",
        "    test_loss, test_acc = evaluate_bigbird(\n",
        "        model_bb, test_loader_bb, criterion, device\n",
        "    )\n",
        "    \n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "    \n",
        "    wandb.log({\n",
        "        'epoch': epoch + 1,\n",
        "        'train/loss': train_loss,\n",
        "        'train/accuracy': train_acc,\n",
        "        'val/loss': test_loss,\n",
        "        'val/accuracy': test_acc,\n",
        "    })\n",
        "    \n",
        "    if test_acc > best_acc_bb:\n",
        "        best_acc_bb = test_acc\n",
        "        torch.save(model_bb.state_dict(), 'bigbird_classifier_best.pt')\n",
        "\n",
        "wandb.finish()\n",
        "print(f\"\\nBest Test Accuracy (BigBird): {best_acc_bb:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Results Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Phase 3 Results: Long Document Classification\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Model':<25} {'Max Length':>12} {'Test Accuracy':>15}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'BERT (Phase 2)':<25} {'512':>12} {'~88%':>15}\")\n",
        "print(f\"{'Longformer':<25} {f'{MAX_LENGTH}':>12} {f'{best_acc*100:.2f}%':>15}\")\n",
        "try:\n",
        "    print(f\"{'BigBird':<25} {f'{MAX_LENGTH}':>12} {f'{best_acc_bb*100:.2f}%':>15}\")\n",
        "except:\n",
        "    print(f\"{'BigBird':<25} {f'{MAX_LENGTH}':>12} {'Not trained':>15}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/hmcan_phase3\n",
        "!cp longformer_classifier_best.pt /content/drive/MyDrive/hmcan_phase3/\n",
        "!cp bigbird_classifier_best.pt /content/drive/MyDrive/hmcan_phase3/ 2>/dev/null || true\n",
        "print(\"Models saved to Google Drive!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Memory & Speed Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def measure_inference_time(model, loader, device, num_batches=10):\n",
        "    model.eval()\n",
        "    times = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(loader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            \n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            \n",
        "            start = time.time()\n",
        "            if 'global_attention_mask' in batch:\n",
        "                global_attention_mask = batch['global_attention_mask'].to(device)\n",
        "                _ = model(input_ids, attention_mask, global_attention_mask)\n",
        "            else:\n",
        "                _ = model(input_ids, attention_mask)\n",
        "            torch.cuda.synchronize()\n",
        "            times.append(time.time() - start)\n",
        "    \n",
        "    return sum(times) / len(times) * 1000  # ms per batch\n",
        "\n",
        "# Measure (if models are loaded)\n",
        "try:\n",
        "    longformer_time = measure_inference_time(model, test_loader, device)\n",
        "    print(f\"Longformer inference: {longformer_time:.2f} ms/batch\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    bigbird_time = measure_inference_time(model_bb, test_loader_bb, device)\n",
        "    print(f\"BigBird inference: {bigbird_time:.2f} ms/batch\")\n",
        "except:\n",
        "    pass"
      ]
    }
  ]
}
