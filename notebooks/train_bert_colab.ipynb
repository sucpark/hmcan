{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT Document Classification on Google Colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sucpark/hmcan/blob/main/notebooks/train_bert_colab.ipynb)\n",
        "\n",
        "## Phase 2: Transformer Era\n",
        "\n",
        "Multi-dataset support with BERT-based models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Configuration\n",
        "\n",
        "**Select your dataset and settings here!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Dataset & Training Configuration { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### Dataset Selection\n",
        "DATASET = \"yelp\" #@param [\"yelp\", \"imdb\", \"ag_news\", \"dbpedia\", \"yahoo\", \"newsgroups\"]\n",
        "\n",
        "#@markdown ### Training Settings\n",
        "MAX_SAMPLES = 10000 #@param {type:\"integer\"}\n",
        "MAX_LENGTH = 512 #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 16 #@param {type:\"integer\"}\n",
        "LEARNING_RATE = 2e-5 #@param {type:\"number\"}\n",
        "NUM_EPOCHS = 3 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Logging\n",
        "USE_WANDB = True #@param {type:\"boolean\"}\n",
        "WANDB_PROJECT = \"hmcan\" #@param {type:\"string\"}\n",
        "\n",
        "# Dataset info\n",
        "DATASET_INFO = {\n",
        "    \"yelp\": {\"name\": \"yelp_review_full\", \"num_classes\": 5, \"text\": \"text\", \"label\": \"label\"},\n",
        "    \"imdb\": {\"name\": \"imdb\", \"num_classes\": 2, \"text\": \"text\", \"label\": \"label\"},\n",
        "    \"ag_news\": {\"name\": \"ag_news\", \"num_classes\": 4, \"text\": \"text\", \"label\": \"label\"},\n",
        "    \"dbpedia\": {\"name\": \"dbpedia_14\", \"num_classes\": 14, \"text\": \"content\", \"label\": \"label\"},\n",
        "    \"yahoo\": {\"name\": \"yahoo_answers_topics\", \"num_classes\": 10, \"text\": \"question_content\", \"label\": \"topic\"},\n",
        "    \"newsgroups\": {\"name\": \"SetFit/20_newsgroups\", \"num_classes\": 20, \"text\": \"text\", \"label\": \"label\"},\n",
        "}\n",
        "\n",
        "config = DATASET_INFO[DATASET]\n",
        "NUM_CLASSES = config[\"num_classes\"]\n",
        "\n",
        "print(f\"Dataset: {DATASET}\")\n",
        "print(f\"Classes: {NUM_CLASSES}\")\n",
        "print(f\"Max samples: {MAX_SAMPLES}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install transformers>=4.30.0 -q\n",
        "!pip install sentence-transformers>=2.2.0 -q\n",
        "!pip install datasets>=2.14.0 -q\n",
        "!pip install wandb -q\n",
        "!pip install accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    BertModel, \n",
        "    BertTokenizer,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import wandb\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Weights & Biases Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if USE_WANDB:\n",
        "    wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset from HuggingFace\n",
        "print(f\"Loading {DATASET} dataset...\")\n",
        "dataset = load_dataset(config[\"name\"])\n",
        "\n",
        "# Get train and test splits\n",
        "train_dataset = dataset['train'].shuffle(seed=42).select(range(min(MAX_SAMPLES, len(dataset['train']))))\n",
        "\n",
        "if 'test' in dataset:\n",
        "    test_dataset = dataset['test'].shuffle(seed=42).select(range(min(MAX_SAMPLES // 10, len(dataset['test']))))\n",
        "else:\n",
        "    # Use validation or split\n",
        "    split = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    train_dataset = split['train']\n",
        "    test_dataset = split['test']\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample\n",
        "sample = train_dataset[0]\n",
        "text_field = config['text']\n",
        "label_field = config['label']\n",
        "\n",
        "print(f\"Sample text: {sample[text_field][:200]}...\")\n",
        "print(f\"Label: {sample[label_field]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. BERT Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class MultiDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, text_field, label_field, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_field = text_field\n",
        "        self.label_field = label_field\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        \n",
        "        # Handle Yahoo's multi-field format\n",
        "        if self.text_field == 'question_content':\n",
        "            title = item.get('question_title', '')\n",
        "            content = item.get('question_content', '')\n",
        "            answer = item.get('best_answer', '')\n",
        "            text = f\"{title} {content} {answer}\".strip()\n",
        "        else:\n",
        "            text = item[self.text_field]\n",
        "        \n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(item[self.label_field])\n",
        "        }\n",
        "\n",
        "train_ds = MultiDataset(train_dataset, tokenizer, config['text'], config['label'], MAX_LENGTH)\n",
        "test_ds = MultiDataset(test_dataset, tokenizer, config['text'], config['label'], MAX_LENGTH)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    \"\"\"BERT for document classification.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(768, num_classes)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, scheduler, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(loader, desc='Training')\n",
        "    for batch in pbar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        \n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{correct/total:.4f}'})\n",
        "    \n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch in tqdm(loader, desc='Evaluating'):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    \n",
        "    return total_loss / len(loader), correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize wandb\n",
        "if USE_WANDB:\n",
        "    wandb.init(\n",
        "        project=WANDB_PROJECT,\n",
        "        name=f'bert-{DATASET}',\n",
        "        config={\n",
        "            'model': 'bert-base-uncased',\n",
        "            'dataset': DATASET,\n",
        "            'num_classes': NUM_CLASSES,\n",
        "            'max_length': MAX_LENGTH,\n",
        "            'batch_size': BATCH_SIZE,\n",
        "            'learning_rate': LEARNING_RATE,\n",
        "            'epochs': NUM_EPOCHS,\n",
        "            'max_samples': MAX_SAMPLES,\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Initialize model\n",
        "model = BERTClassifier(num_classes=NUM_CLASSES).to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "total_steps = len(train_loader) * NUM_EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "best_acc = 0\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    \n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, criterion, device)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "    \n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "    \n",
        "    if USE_WANDB:\n",
        "        wandb.log({\n",
        "            'epoch': epoch + 1,\n",
        "            'train/loss': train_loss,\n",
        "            'train/accuracy': train_acc,\n",
        "            'val/loss': test_loss,\n",
        "            'val/accuracy': test_acc,\n",
        "        })\n",
        "    \n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save(model.state_dict(), f'bert_{DATASET}_best.pt')\n",
        "        print(f\"Saved best model with accuracy: {best_acc:.4f}\")\n",
        "\n",
        "if USE_WANDB:\n",
        "    wandb.finish()\n",
        "\n",
        "print(f\"\\nBest Test Accuracy: {best_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Run All Datasets (Optional)\n",
        "\n",
        "Uncomment and run to train on all datasets sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Uncomment to run on all datasets\n",
        "# ALL_DATASETS = [\"yelp\", \"imdb\", \"ag_news\", \"dbpedia\", \"yahoo\", \"newsgroups\"]\n",
        "# all_results = {}\n",
        "# \n",
        "# for ds_name in ALL_DATASETS:\n",
        "#     print(f\"\\n{'='*60}\")\n",
        "#     print(f\"Training on {ds_name}\")\n",
        "#     print(f\"{'='*60}\")\n",
        "#     \n",
        "#     ds_config = DATASET_INFO[ds_name]\n",
        "#     \n",
        "#     # Load dataset\n",
        "#     dataset = load_dataset(ds_config[\"name\"])\n",
        "#     train_data = dataset['train'].shuffle(seed=42).select(range(min(5000, len(dataset['train']))))\n",
        "#     test_data = dataset['test'].shuffle(seed=42).select(range(min(500, len(dataset['test']))))\n",
        "#     \n",
        "#     # Create data loaders\n",
        "#     train_ds = MultiDataset(train_data, tokenizer, ds_config['text'], ds_config['label'], 512)\n",
        "#     test_ds = MultiDataset(test_data, tokenizer, ds_config['text'], ds_config['label'], 512)\n",
        "#     train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "#     test_loader = DataLoader(test_ds, batch_size=16)\n",
        "#     \n",
        "#     # Initialize model\n",
        "#     model = BERTClassifier(num_classes=ds_config['num_classes']).to(device)\n",
        "#     optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "#     scheduler = get_linear_schedule_with_warmup(optimizer, 0, len(train_loader) * 2)\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     \n",
        "#     # Train for 2 epochs\n",
        "#     for epoch in range(2):\n",
        "#         train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, criterion, device)\n",
        "#         test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "#     \n",
        "#     all_results[ds_name] = test_acc\n",
        "#     print(f\"{ds_name}: {test_acc*100:.2f}%\")\n",
        "#     \n",
        "#     # Clear memory\n",
        "#     del model, optimizer, scheduler\n",
        "#     torch.cuda.empty_cache()\n",
        "# \n",
        "# # Print summary\n",
        "# print(f\"\\n{'='*60}\")\n",
        "# print(\"Results Summary\")\n",
        "# print(f\"{'='*60}\")\n",
        "# for ds_name, acc in all_results.items():\n",
        "#     print(f\"{ds_name:<15}: {acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/hmcan_phase2\n",
        "!cp bert_*_best.pt /content/drive/MyDrive/hmcan_phase2/ 2>/dev/null || true\n",
        "print(\"Models saved to Google Drive!\")"
      ]
    }
  ]
}
