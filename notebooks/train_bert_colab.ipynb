{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Document Classification (VS Code + Colab Extension)\n",
    "\n",
    "## Phase 2: Transformer Era\n",
    "\n",
    "Multi-dataset support with BERT-based models.\n",
    "\n",
    "**Compatible with VS Code Colab Extension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**Select your dataset and settings here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dataset Selection ===\n",
    "DATASET = \"yelp\"  # Options: \"yelp\", \"imdb\", \"ag_news\", \"dbpedia\", \"yahoo\", \"newsgroups\"\n",
    "\n",
    "# === Training Settings ===\n",
    "MAX_SAMPLES = 10000\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# === Logging Settings ===\n",
    "USE_WANDB = False  # Set True to enable WandB logging\n",
    "WANDB_API_KEY = \"\"  # Paste your API key here if using WandB\n",
    "WANDB_PROJECT = \"hmcan\"\n",
    "\n",
    "# Dataset info\n",
    "DATASET_INFO = {\n",
    "    \"yelp\": {\"name\": \"yelp_review_full\", \"num_classes\": 5, \"text\": \"text\", \"label\": \"label\"},\n",
    "    \"imdb\": {\"name\": \"imdb\", \"num_classes\": 2, \"text\": \"text\", \"label\": \"label\"},\n",
    "    \"ag_news\": {\"name\": \"ag_news\", \"num_classes\": 4, \"text\": \"text\", \"label\": \"label\"},\n",
    "    \"dbpedia\": {\"name\": \"dbpedia_14\", \"num_classes\": 14, \"text\": \"content\", \"label\": \"label\"},\n",
    "    \"yahoo\": {\"name\": \"yahoo_answers_topics\", \"num_classes\": 10, \"text\": \"question_content\", \"label\": \"topic\"},\n",
    "    \"newsgroups\": {\"name\": \"SetFit/20_newsgroups\", \"num_classes\": 20, \"text\": \"text\", \"label\": \"label\"},\n",
    "}\n",
    "\n",
    "config = DATASET_INFO[DATASET]\n",
    "NUM_CLASSES = config[\"num_classes\"]\n",
    "\n",
    "print(f\"Dataset: {DATASET}\")\n",
    "print(f\"Classes: {NUM_CLASSES}\")\n",
    "print(f\"Max samples: {MAX_SAMPLES}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers>=4.30.0 -q\n",
    "!pip install sentence-transformers>=2.2.0 -q\n",
    "!pip install datasets>=2.14.0 -q\n",
    "!pip install wandb -q\n",
    "!pip install accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    BertModel, \n",
    "    BertTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Weights & Biases Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB setup (programmatic login - no interactive prompt)\n",
    "if USE_WANDB and WANDB_API_KEY:\n",
    "    import wandb\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "    print(\"WandB logged in successfully!\")\n",
    "elif USE_WANDB:\n",
    "    print(\"Warning: USE_WANDB=True but no API key provided. WandB will be disabled.\")\n",
    "    USE_WANDB = False\n",
    "else:\n",
    "    print(\"WandB disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from HuggingFace\n",
    "print(f\"Loading {DATASET} dataset...\")\n",
    "dataset = load_dataset(config[\"name\"])\n",
    "\n",
    "# Get train and test splits\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(min(MAX_SAMPLES, len(dataset['train']))))\n",
    "\n",
    "if 'test' in dataset:\n",
    "    test_dataset = dataset['test'].shuffle(seed=42).select(range(min(MAX_SAMPLES // 10, len(dataset['test']))))\n",
    "else:\n",
    "    # Use validation or split\n",
    "    split = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = split['train']\n",
    "    test_dataset = split['test']\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample\n",
    "sample = train_dataset[0]\n",
    "text_field = config['text']\n",
    "label_field = config['label']\n",
    "\n",
    "print(f\"Sample text: {sample[text_field][:200]}...\")\n",
    "print(f\"Label: {sample[label_field]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. BERT Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MultiDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, text_field, label_field, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_field = text_field\n",
    "        self.label_field = label_field\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Handle Yahoo's multi-field format\n",
    "        if self.text_field == 'question_content':\n",
    "            title = item.get('question_title', '')\n",
    "            content = item.get('question_content', '')\n",
    "            answer = item.get('best_answer', '')\n",
    "            text = f\"{title} {content} {answer}\".strip()\n",
    "        else:\n",
    "            text = item[self.text_field]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(item[self.label_field])\n",
    "        }\n",
    "\n",
    "train_ds = MultiDataset(train_dataset, tokenizer, config['text'], config['label'], MAX_LENGTH)\n",
    "test_ds = MultiDataset(test_dataset, tokenizer, config['text'], config['label'], MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    \"\"\"BERT for document classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{correct/total:.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Evaluating'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb if enabled\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        name=f'bert-{DATASET}',\n",
    "        config={\n",
    "            'model': 'bert-base-uncased',\n",
    "            'dataset': DATASET,\n",
    "            'num_classes': NUM_CLASSES,\n",
    "            'max_length': MAX_LENGTH,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'epochs': NUM_EPOCHS,\n",
    "            'max_samples': MAX_SAMPLES,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Initialize model\n",
    "model = BERTClassifier(num_classes=NUM_CLASSES).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "best_acc = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, criterion, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train/loss': train_loss,\n",
    "            'train/accuracy': train_acc,\n",
    "            'val/loss': test_loss,\n",
    "            'val/accuracy': test_acc,\n",
    "        })\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), f'bert_{DATASET}_best.pt')\n",
    "        print(f\"Saved best model with accuracy: {best_acc:.4f}\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n",
    "\n",
    "print(f\"\\nBest Test Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run All Datasets (Optional)\n",
    "\n",
    "Uncomment and run to train on all datasets sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to run on all datasets\n",
    "# ALL_DATASETS = [\"yelp\", \"imdb\", \"ag_news\", \"dbpedia\", \"yahoo\", \"newsgroups\"]\n",
    "# all_results = {}\n",
    "# \n",
    "# for ds_name in ALL_DATASETS:\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Training on {ds_name}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     \n",
    "#     ds_config = DATASET_INFO[ds_name]\n",
    "#     \n",
    "#     # Load dataset\n",
    "#     dataset = load_dataset(ds_config[\"name\"])\n",
    "#     train_data = dataset['train'].shuffle(seed=42).select(range(min(5000, len(dataset['train']))))\n",
    "#     test_data = dataset['test'].shuffle(seed=42).select(range(min(500, len(dataset['test']))))\n",
    "#     \n",
    "#     # Create data loaders\n",
    "#     train_ds = MultiDataset(train_data, tokenizer, ds_config['text'], ds_config['label'], 512)\n",
    "#     test_ds = MultiDataset(test_data, tokenizer, ds_config['text'], ds_config['label'], 512)\n",
    "#     train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "#     test_loader = DataLoader(test_ds, batch_size=16)\n",
    "#     \n",
    "#     # Initialize model\n",
    "#     model = BERTClassifier(num_classes=ds_config['num_classes']).to(device)\n",
    "#     optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "#     scheduler = get_linear_schedule_with_warmup(optimizer, 0, len(train_loader) * 2)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     \n",
    "#     # Train for 2 epochs\n",
    "#     for epoch in range(2):\n",
    "#         train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, criterion, device)\n",
    "#         test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "#     \n",
    "#     all_results[ds_name] = test_acc\n",
    "#     print(f\"{ds_name}: {test_acc*100:.2f}%\")\n",
    "#     \n",
    "#     # Clear memory\n",
    "#     del model, optimizer, scheduler\n",
    "#     torch.cuda.empty_cache()\n",
    "# \n",
    "# # Print summary\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"Results Summary\")\n",
    "# print(f\"{'='*60}\")\n",
    "# for ds_name, acc in all_results.items():\n",
    "#     print(f\"{ds_name:<15}: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results\n",
    "\n",
    "**Option 1: Download locally** (VS Code Colab - files remain on remote)\n",
    "\n",
    "**Option 2: Git Push** (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved models\n",
    "!ls -la *.pt 2>/dev/null || echo \"No model files found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Clone repo and save models there for git push\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/sucpark/hmcan.git\"\n",
    "PROJECT_DIR = \"/content/hmcan\"\n",
    "\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    !git clone {REPO_URL} {PROJECT_DIR}\n",
    "\n",
    "# Copy model to repo\n",
    "!mkdir -p {PROJECT_DIR}/outputs/bert_phase2\n",
    "!cp bert_*_best.pt {PROJECT_DIR}/outputs/bert_phase2/ 2>/dev/null || echo \"No models to copy\"\n",
    "\n",
    "print(f\"Models copied to {PROJECT_DIR}/outputs/bert_phase2/\")\n",
    "print(\"To push: cd to repo, git add, commit, push\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
